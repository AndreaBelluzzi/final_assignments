{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f54dd50c",
   "metadata": {
    "id": "f54dd50c"
   },
   "source": [
    "# Assignments - module 1\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vigji/python-cimec/blob/main/assignments/Assignments_1.ipynb)\n",
    "\n",
    "This notebook contains the assignments to complete for credits for the first module.\n",
    "\n",
    "**Submission**: Once you're happy with your solutions, send it to me in any form (email the file, share it through Colab/Google Drive, send me a link to your GitHub repo...).\n",
    "\n",
    "**Deadline**: 15th of July 2023\n",
    "\n",
    "**Evaluation**: There is no grade, but I will pass assignments that showcase a reasonable degree of understanding og the covered topics. Do your best, and feel free to ask for help if you are struggling!\n",
    "\n",
    "(Also, try to keep in mind not only the goal of the exercise, but also all the coding best practices we have been considering in the lectures.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c41f5f",
   "metadata": {
    "id": "f3c41f5f"
   },
   "source": [
    "## 0. Spike detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa4168",
   "metadata": {
    "id": "f5fa4168"
   },
   "source": [
    "In this exercise we will be playing around with some (dummy) electrophysiology recordings. Let's start by having a look at the raw data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070113ec",
   "metadata": {
    "id": "070113ec"
   },
   "outputs": [],
   "source": [
    "def generate_spike_trace(trace_length=60, firing_rate=1, noise_sigma = 0.05):\n",
    "    \"\"\"Function to generate a fake extracellular recording.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        trace_length : float\n",
    "            Duration of the recording in seconds.\n",
    "\n",
    "        firing_rate : float\n",
    "            Average firing rate of the neuron in Hz.\n",
    "\n",
    "        noise_sigma : float\n",
    "            Noise level.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        np.array\n",
    "            Fake extracellular recording.\n",
    "\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    FS = 10000  # sampling frequency\n",
    "    n = int(trace_length * FS)  # number of samples\n",
    "\n",
    "    # Generate spike shape template as a difference of Gaussians.\n",
    "    # A horrible bunch of magic numbers - do not imitate!\n",
    "    x = np.arange(30)\n",
    "    spike_template = np.exp(-(x - 10)**2/6) - np.exp(-(x - 12)**2/16)*0.8\n",
    "\n",
    "    # Generate spike times from a gaussian distribution:\n",
    "    spikes_times = np.random.poisson(firing_rate / FS, n)\n",
    "\n",
    "    # Convolve dirac delta functions of spike times with spike template:\n",
    "    trace = np.convolve(spikes_times, spike_template)[:n]\n",
    "\n",
    "    # Add some gaussian noise:\n",
    "    trace += np.random.normal(0, noise_sigma, n)\n",
    "\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30406abd",
   "metadata": {
    "id": "30406abd"
   },
   "source": [
    "### Exercise 0.0\n",
    "\n",
    "Run the function below to generate an synthetic extracellular recording for a neuron. Make a nice plot with the trace; the spikes are the peaks appearing above the noise!\n",
    "\n",
    "---\n",
    "\n",
    "(_Optional_) If you want to make a plot with exact x coordinates in seconds, you should know that the trace is sampled at 10000 Hz (10000 points per second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CPe09KP4oP5r",
   "metadata": {
    "id": "CPe09KP4oP5r"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773e63cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "773e63cb",
    "outputId": "6a24c497-282e-4095-fa1b-ea8aca8b48a8"
   },
   "outputs": [],
   "source": [
    "spike_trace = generate_spike_trace()\n",
    "sampling_rate = 10000 #Hz\n",
    "\n",
    "seconds = len(spike_trace)/sampling_rate  #Secondi di recoding\n",
    "\n",
    "seconds_axisX = np.linspace(0,seconds, num = len(spike_trace))  #x axis\n",
    "plt.plot(seconds_axisX, spike_trace, 'blue', linewidth = 0.2)\n",
    "\n",
    "plt.xlabel('Time (s)')\n",
    "plt.title('synthetic extracellular recording - single neuron')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4424605",
   "metadata": {
    "id": "b4424605"
   },
   "source": [
    "### Exercise 0.1\n",
    "\n",
    "Write a function to detect spikes!\n",
    "The function should take the trace as input, and return the index of each spike as the output (as the index, you should take the position of the spike maximum)\n",
    "\n",
    "Hint: a good strategy to detect such events is to set a threshold, and look for elements above it. This will not be enough! each spike could have more than 1 point above the threshold, but you want to make sure you take only the spike peak! For this, you will probably need a loop.\n",
    "\n",
    "Hint: do not start from writing the function. First debug your code running it in a cell, then move it to a function.\n",
    "\n",
    "Hint: if you want, you can quickly check out the results you are getting by making a scatter plot of the detected spikes overimposed on the electrophysiology trace! (use as x of the dots the indexes of the spikes, and as y the hight of the trace at those indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539af978",
   "metadata": {
    "id": "539af978"
   },
   "outputs": [],
   "source": [
    "\n",
    "def peaks_buster(data):  # LP good name!  :)\n",
    "  \"\"\" Function for finding peaks\n",
    "      peaks = magnitude of the peak - y value\n",
    "      idx_peaks = when that peaks happened - x value  \"\"\"\n",
    "\n",
    "  THR = 0.25 #threshold\n",
    "  peaks = []\n",
    "  idx_peaks = []\n",
    "\n",
    "  past_point = 0 #\n",
    "\n",
    "  for idx, value in enumerate(data):\n",
    "\n",
    "    if idx == 0:\n",
    "      past_point = 0\n",
    "    else:\n",
    "      past_point = data[idx-1]\n",
    "\n",
    "    actual_point =  value       # datapoint preso in esame ora\n",
    "    if idx < (len(data)-1):   # controllare che non siamo arrivati alla fine di data\n",
    "      next_point = data[idx+1]\n",
    "    else:\n",
    "      next_point = 0            # ultimo datapoint\n",
    "\n",
    "    if  (actual_point > THR) & (actual_point > next_point) & (actual_point > past_point) :  #controllare se supera THR ed è effettivamente più alto dei 2 punti vicini\n",
    "      peaks.append(value)\n",
    "      idx_peaks.append(idx)\n",
    "\n",
    "  return peaks, idx_peaks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G6EihEibuwS3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "G6EihEibuwS3",
    "outputId": "858d4f54-d0ef-41fd-930a-6dc08d5e234d"
   },
   "outputs": [],
   "source": [
    "peaks, idx_peaks = peaks_buster(spike_trace)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(spike_trace, 'blue', linewidth = 0.2)\n",
    "plt.plot(idx_peaks, peaks, 'rD', markersize = 3)\n",
    "\n",
    "plt.xlabel('Time (s)')\n",
    "plt.legend(['spike', 'peaks'],bbox_to_anchor=(1.05, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5b57a",
   "metadata": {
    "id": "81e5b57a"
   },
   "source": [
    "### Exercise 0.2\n",
    "\n",
    "We now want to have a look at the shape of those spikes. For this, we will create a function that crops small chunks of the trace around each spike peak.\n",
    "\n",
    "Write a `crop_event()` function that takes as inputs:\n",
    "   - the recording array\n",
    "   - the spike indexes\n",
    "   - a `n_points_pad` variable specifying the number of points to include before and after the spike\n",
    "\n",
    "And returns a matrix of shape `(n_spikes, n_points*2)` containing the trace chunks cropped around spike events!\n",
    "\n",
    "Hint: A good strategy coult be initialize an empty matrix and then fill it in a loop with the trace around the spikes.\n",
    "\n",
    "**This function can be very useful in many contexts!** You can use it every time you want to crop a timeseries around events (e.g., EEG data or video kinematics data around some stimuli). So keep it at hand in the future!\n",
    "\n",
    "---\n",
    "\n",
    "(_Optional_) Pro challenge: Try to do it without for loops! if you construct a matrix with the indexes of the points you want to exctract from the trace, you can use it directly to index the trace!\n",
    "For indexing in this way, you want to build a matrix that looks like this:\n",
    "```\n",
    "array([[...t0-2, t0-1, t0, t0+1, t0+2...],\n",
    "       [...t1-2, t1-1, t1, t1+1, t1+2...],\n",
    "       [...t2-2, t2-1, t2, t2+1, t2+2...],])\n",
    "```\n",
    "Where `t0`, `t1`, `t2`... are the indexes of each spike, and you take as many points before and after as specified by the `n_points_pad` paramenter.\n",
    "\n",
    "Building this matrix without loops is not trivial but it can be done nicely with numpy broadcasting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cbdba7",
   "metadata": {
    "id": "53cbdba7"
   },
   "outputs": [],
   "source": [
    "n_points_pad = 3\n",
    "\n",
    "def crop_event(spike_trace, idx_peaks, n_points_pad):\n",
    "  \"\"\" spike_trace = data to crop\n",
    "      idx_peaks = when the peaks happened\n",
    "      n_points_pad = how many data points after and before each peak you want to crop\n",
    "  \"\"\"\n",
    "\n",
    "  real_n_points_pad = (n_points_pad*2)+1    #se n_points_pad = 3, vuol dire da -3 a +3, a step di 1, quindi 7 step\n",
    "  chunks_peaks = np.zeros((len(idx_peaks), (n_points_pad*2))) #matrice vuota\n",
    "\n",
    "  points_close_peaks_float = np.linspace(n_points_pad , (n_points_pad*-1), num = real_n_points_pad)\n",
    "  points_close_peaks = points_close_peaks_float.astype(int)  #conversione da float ad integer\n",
    "\n",
    "  #Se non voglio far comparire nel crop anche il valore originale, devo rimuovere lo zero da \"points_close_peaks\"\n",
    "  mask_zero = (points_close_peaks!=0)  #mask per selezionare tutto ciò che non è zero\n",
    "  points_close_peaks = points_close_peaks[mask_zero]  #rimuovere zero\n",
    "  #print(points_close_peaks)\n",
    "\n",
    "  for idx , value_spike in enumerate(idx_peaks):\n",
    "    for idx_point , value_point in enumerate(points_close_peaks):\n",
    "      spike_to_grab = (value_spike - value_point)\n",
    "      chunks_peaks[idx, idx_point] = spike_trace[spike_to_grab]\n",
    "\n",
    "      #print(spike_to_grab) #i 3 spike precedenti e poi i 3 che seguono\n",
    "\n",
    "  return(chunks_peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4b3f8-88fd-4457-a1f9-20fccbbcba87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WUcS0yTD6fUp",
    "outputId": "c30d1a8e-5861-453b-8d50-b5bc13674324"
   },
   "outputs": [],
   "source": [
    "cropped_data = crop_event(spike_trace,idx_peaks,3)\n",
    "cropped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14949dca",
   "metadata": {
    "id": "14949dca"
   },
   "source": [
    "### Exercise 0.3\n",
    "\n",
    "Finally, make two subplots one close to the other. On the left, use `plt.matshow` to show the spike matrix. On the right,\n",
    "plot each individual spike (rows of the matrix) using `plt.plot` with gray lines, and the average spike shape in red on top.\n",
    "\n",
    "---\n",
    "\n",
    "(Optional) If you want you can try to normalize the matrix before plotting by subtracting the average of each row (as we were doing for the daily temperatures)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f15fdd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 739
    },
    "id": "40f15fdd",
    "outputId": "124fe5fe-ddad-4629-d4d6-3f0df13ea552"
   },
   "outputs": [],
   "source": [
    "# LP: Something's funny - Idk why but you seem to not always find maxima, otherwise peaks should always be at the same\n",
    "# position in the plot!\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (10, 8)) # create two subplots\n",
    "\n",
    "# primo subplot\n",
    "im = axes[0].matshow(cropped_data)  #spike matrix\n",
    "cbar = fig.colorbar(im, ax=axes[0])\n",
    "axes[0].set(title = 'spike matrix')\n",
    "\n",
    "im.set_cmap('coolwarm')\n",
    "max_value_spike= np.max(np.abs(cropped_data))\n",
    "im.set_clim(vmin=-max_value_spike, vmax=max_value_spike)  #mettiamo lo stesso range per la colormap\n",
    "\n",
    "# second subplot\n",
    "for row in cropped_data:\n",
    "  axes[1].plot(row, color = 'grey')   # individual spike\n",
    "\n",
    "column_means = np.mean(cropped_data, axis = 0)\n",
    "axes[1].plot(column_means, color = 'r')   # mean spike\n",
    "axes[1].set(title = 'individual and mean spike')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1d075",
   "metadata": {
    "id": "dfa1d075"
   },
   "source": [
    "## 1. Real books data\n",
    "\n",
    "After having appreciated how many books the universe of all possible books contains, let's now focus just on the reachable ones - and how much people like them!\n",
    "\n",
    "Here, we will download the information about about thousands volumns available on Amazon. Just a tiny fraction of Babel's books, but way more organized!\n",
    "\n",
    "We will also get a dataset of users writing reviews, and a dataset of reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b36e168",
   "metadata": {
    "id": "5b36e168"
   },
   "source": [
    "### Exercise 1.0\n",
    "\n",
    "Using, `pandas`, read the `.csv` files containing the books, the ratings, and the user data that you can find at the  urls defined below.\n",
    "\n",
    "Then, plot an histogram of all the ratings from all users, and another histogram with the age of the users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b17ed3",
   "metadata": {
    "id": "f9b17ed3"
   },
   "outputs": [],
   "source": [
    "users_df_url = \"https://github.com/vigji/python-cimec/raw/main/assignments/files/users.csv\"\n",
    "ratings_df_url = \"https://github.com/vigji/python-cimec/raw/main/assignments/files/ratings.csv\"\n",
    "books_df_url = \"https://github.com/vigji/python-cimec/raw/main/assignments/files/books.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Edqv8joJadKo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "Edqv8joJadKo",
    "outputId": "516e4337-0f2b-4799-8d21-90f77442268d"
   },
   "outputs": [],
   "source": [
    "users = pd.read_csv(users_df_url)\n",
    "ratings = pd.read_csv(ratings_df_url)\n",
    "books = pd.read_csv(books_df_url)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (10, 6))\n",
    "\n",
    "#Ratings\n",
    "ax[0].hist(ratings['rating'], color = 'seagreen')\n",
    "ax[0].set(xlabel = \"score\", title = \"Ratings\")\n",
    "\n",
    "#Age\n",
    "ax[1].hist(users['age'] , color = 'royalblue')\n",
    "ax[1].set(xlabel = \"year\", title = \"Age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3c5799",
   "metadata": {
    "id": "4e3c5799"
   },
   "source": [
    "### Exercise 1.1\n",
    "\n",
    "Using the ratings dataframe, compute the average rating for each book, and count how many reviews each book had. Then:\n",
    "- find out which book had the highest number of reviews.\n",
    "- find out which book had the highest average rating - but include only books that have at least 100 reviews!\n",
    "\n",
    "\n",
    "Finally, look for the titles that correspond to those book codes (ISBNs are unique book codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PxI6bG4Dc6Il",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PxI6bG4Dc6Il",
    "outputId": "896a5e51-eb6b-459d-ea29-4b37f46e5af6"
   },
   "outputs": [],
   "source": [
    "book_counts = ratings['ISBN'].value_counts()\n",
    "\n",
    "#highest number of review\n",
    "N_review = book_counts.max()\n",
    "most_reviewed_ISBN = book_counts.idxmax()\n",
    "\n",
    "#Given ISBN code, find the title in books dataframe\n",
    "mask = books['ISBN']==most_reviewed_ISBN\n",
    "# LP: most times (always?) values returns arrays, we're to make peace with that...\n",
    "title_most_reviewed_book = books.loc[mask , 'title'].values[0]  #Non ho ben capito perchè debba usare anche .values[0] in modo da selezionare solo il titolo. Se non lo faccio title_most_reviewed_book mi riporta anche la riga del dataframe in cui c'è quel codice ISBN\n",
    "\n",
    "print(f\"The most reviewed book is '{title_most_reviewed_book}', with {N_review} reviews. Its ISBN code is {most_reviewed_ISBN} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Tr-gT0UONpm3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tr-gT0UONpm3",
    "outputId": "ffc2655e-9cd7-48df-8f55-beb396fe4063"
   },
   "outputs": [],
   "source": [
    "avg_rating = ratings.groupby('ISBN').mean()\n",
    "\n",
    "# LP: kind of what I'm reading now maybe\n",
    "#Top book with highest avg rating - we don't care about \"mattone polacco minimalista di scrittore morto suicida giovanissimo\" with less then 100 reviews\n",
    "mask_book_100 = book_counts.loc[book_counts > 100]  # Good but define this magic number!\n",
    "\n",
    "# LP: \n",
    "# LP: Super-duper important!! You gave me the example for next iteration of the course of why NEVER \n",
    "# using iloc unless you are ABSOLUTELY sure of what you are doing (aka, don't use it):\n",
    "# It assumes that your dataframes are sorted in exactly the same way, but they not always are!\n",
    "# In this case, you have found a mask over one set, but to apply it you should mask semantical indexes (ISBN codes)\n",
    "# NOT absolute indexes!\n",
    "# Compare your result with what you get with \n",
    "\n",
    "# book_100 = avg_rating.loc[mask_book_100.index]\n",
    "\n",
    "# And see that the result will look more plausible :)\n",
    "\n",
    "book_100 = avg_rating.iloc[mask_book_100]\n",
    "\n",
    "highest_avg_rating = book_100['rating'].max()\n",
    "highest_avg_rating_ISBN = book_100['rating'].idxmax()\n",
    "\n",
    "#Given ISBN code, find the title in books dataframe - same code as above\n",
    "mask2 = books['ISBN']==highest_avg_rating_ISBN\n",
    "title_top_book = books.loc[mask2 , 'title'].values[0]\n",
    "\n",
    "print(f\"The book with the highest average is '{title_top_book}'. Its ISBN code is {highest_avg_rating_ISBN} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1025ba6a-de70-435b-b170-4154eb3ce898",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b33356",
   "metadata": {
    "id": "e9b33356"
   },
   "source": [
    "### Exercise 1.2\n",
    "\n",
    "Let's get even more specific! Let's find the preferences of users in specific countries and with different ages.\n",
    "\n",
    "Use the users DataFrame to select only italian users under 40 years old. Then, go back to the reviews dataframe and filter only reviews from those users. Compute the average ratings (include only books that have at least 3 reviews) and sort the ISBNs by average rating. Finally, find the books corresponding to each ISBN code to get which books got the best ratings in this coort of people!\n",
    "\n",
    "(_Optional_): from the users DataFrame generate a list of all the countries present in the dataset. Then, find the highest rated book in each one of those countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ef3e0c",
   "metadata": {
    "id": "f9ef3e0c"
   },
   "outputs": [],
   "source": [
    "#Italians under 40\n",
    "mask_young_italians = (users['country'] == \"italy\") & (users['age'] < 40)\n",
    "young_italians = users.loc[mask_young_italians ]\n",
    "\n",
    "#Italians' ratings under 40\n",
    "mask_italians_ID = ratings['user_id'].isin(young_italians['user_id'])\n",
    "ratings_italians_all = ratings[mask_italians_ID]\n",
    "\n",
    "N_reviews_ita_all = ratings_italians_all['ISBN'].value_counts()   #libri con codice ISBN e corrispondenti review da italians\n",
    "N_reviews_ita = N_reviews_ita_all.loc[N_reviews_ita_all > 2]   #teniamo solo i libri con almeno 3 review\n",
    "\n",
    "mask = ratings_italians_all['ISBN'].isin(N_reviews_ita.index)\n",
    "ratings_italians = ratings_italians_all.loc[mask]   #singole review, tenendo però solo le review per libri con almeno 3 review. Ho già detto review?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7AdU1vdybeb3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "id": "7AdU1vdybeb3",
    "outputId": "e0c405a4-ac11-44e7-ab32-f5934a4aaf20"
   },
   "outputs": [],
   "source": [
    "avg_ratings_italians = ratings_italians.groupby('ISBN').mean()  #media recensioni per ogni libro\n",
    "ranking_books = avg_ratings_italians.sort_values(by= 'rating' , ascending=False) #Dal più alto al più basso in termini di media\n",
    "\n",
    "#Dal dataframe books prendere i libri che corrispondono al codice ISBN\n",
    "# LP: here you filter differently, and you actually don't encounter the issue above\n",
    "mask_books = books['ISBN'].isin(ranking_books.index)\n",
    "top_book_italy = books.loc[mask_books, :]\n",
    "\n",
    "top_book_italy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hlkjELViUB5t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "id": "hlkjELViUB5t",
    "outputId": "ddf6a25b-9f51-4cd3-e0fa-6ab468e78d1e"
   },
   "outputs": [],
   "source": [
    "# LP: No foos!!!\n",
    "foo_df = pd.merge(top_book_italy, ranking_books , on = 'ISBN') #Uniamo il dataframe dei top book italy, con i rispettivi rating\n",
    "\n",
    "#Voglio vedere anche quante recensioni ha ogni singolo libro\n",
    "# LP: good check!\n",
    "df_N_review = pd.DataFrame({'N_review':N_reviews_ita})  #prendi \"N_review_ita\" e convertilo in un dataframe\n",
    "df_N_review['ISBN'] = df_N_review.index   #prendi i suoi index (che sono i codici ISBN) e aggiungilo in una nuova colonna di value\n",
    "\n",
    "#Findal df con libri, relativa media recensioni, e numero di review\n",
    "# LP: In my pandas version I have to drop an ISBN column here.\n",
    "final_df = pd.merge(foo_df, df_N_review , on = 'ISBN')\n",
    "final_df = final_df.drop (columns = ['ISBN' , 'user_id'])  #Facciamo un pò di pulizia\n",
    "final_df = final_df.sort_values(by= 'rating' , ascending=False)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ef6b7-424b-4342-8c04-ccce341602e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb334f8-195f-452a-9121-e87a1ec5bb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:course_env]",
   "language": "python",
   "name": "conda-env-course_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
